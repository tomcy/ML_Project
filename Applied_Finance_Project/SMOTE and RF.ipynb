{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Minority Over-sampling Technique\n",
    "let me use the following example to illustrate how to use SMOTE in unbalance data and prove that the result after SMOTE is better than using original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read neccessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, f1_score, cohen_kappa_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "# read training data and testing data\n",
    "train_data = pd.read_csv(\"training_data.csv\")\n",
    "test_data = pd.read_csv(\"testing_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we use logistic regression on the training dataset and test the trained model in testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy rate in training dataset is  0.764872521246\n"
     ]
    }
   ],
   "source": [
    "# try only logistic regression\n",
    "X = train_data.iloc[:, :-1]\n",
    "y = train_data.iloc[:, -1]\n",
    "# normalize data before regression\n",
    "X = preprocessing.scale(X)\n",
    "c = [0.0001, 0.01, 1, 100]\n",
    "logreg = LogisticRegressionCV(penalty='l2', solver='sag', Cs=c, refit=True, cv=10, max_iter=100)\n",
    "logreg.fit(X, y)\n",
    "print(\"The accuracy rate in training dataset is \", logreg.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result in testing dataset is:\n",
      "Accuracy is  0.521292217327 and recall is 0.316103379722\n",
      "AUC score is  0.47877345936\n"
     ]
    }
   ],
   "source": [
    "# compare the accuracy rate in testing dataset\n",
    "X_test = test_data.iloc[:, :-1]\n",
    "y_test = test_data.iloc[:, -1]\n",
    "y_test_predict = logreg.predict(X_test)\n",
    "print(\"The result in testing dataset is:\")\n",
    "print(\"Accuracy is \", accuracy_score(y_test, y_test_predict), \"and recall is\",recall_score(y_test, y_test_predict))\n",
    "print(\"AUC score is \", roc_auc_score(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the accuracy rate dramaticlly drops on testing dataset means that we overfitting the training data.  \n",
    "SMOTE is Synthetic Minority Over-sampling Technique which can help us address with unbalance dataset. For example, in our history data the number of good record ('1-30 DPD', 'Current', 'Paid', 'Matured') are 3167 and the number of bad record ('Balance Owed', 'Assigned for Repossession', 'Recovered', '90+ DPD', 'Bankruptcy') are 1805. There are several teniques to solve unbalanced dataset in machine learning. SMOTE is one of them. For more information, you may read https://github.com/scikit-learn-contrib/imbalanced-learn. In the following example, I will show how much improvement after we applied SMOTE on logistic regression and random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size before generating:  (4539, 1050)\n",
      "The size after generating: (5830, 1050)\n"
     ]
    }
   ],
   "source": [
    "# use SMOTE to generate synthetic oversampling dataset \n",
    "from imblearn.over_sampling import SMOTE\n",
    "X = pd.concat([train_data.iloc[:, :-1], test_data.iloc[:, :-1]])\n",
    "X = preprocessing.scale(X)\n",
    "y = pd.concat([train_data.iloc[:, -1], test_data.iloc[:, -1]])\n",
    "# generate synthetic dataset\n",
    "sm = SMOTE(kind='regular')\n",
    "X_resampled, y_resampled = sm.fit_sample(X, y)\n",
    "# the date size before and after SMOTE\n",
    "print(\"The size before generating: \", X.shape)\n",
    "print(\"The size after generating:\", X_resampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we use SMOTE to generate dataset, we now applied it on our training model and test on testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result in testing dataset is:\n",
      "accuracy is  0.685534591195 and recall is 0.731934731935\n",
      "AUC score is  0.686393853061\n"
     ]
    }
   ],
   "source": [
    "# reuse logistic regression to train the model and test on testing dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_resampled = pd.DataFrame(X_resampled)\n",
    "y_resampled = pd.DataFrame(y_resampled)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.3, random_state = 42)\n",
    "c = [0.01, 0.1, 1, 10]\n",
    "logreg = LogisticRegressionCV(penalty='l2', solver='sag', Cs=c, refit=True, cv=10, max_iter=100)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_test_predict = logreg.predict(X_test)\n",
    "print(\"The result in testing dataset is:\")\n",
    "print(\"accuracy is \", accuracy_score(y_test, y_test_predict), \"and recall is\",recall_score(y_test, y_test_predict))\n",
    "print(\"AUC score is \", roc_auc_score(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we applied random forest on the data and test the trained model on testing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result in testing dataset is:\n",
      "accuracy is  0.758719268153 and recall is 0.765734265734\n",
      "AUC score is  0.758849175516\n"
     ]
    }
   ],
   "source": [
    "# use random forest to train the model and test on testing dataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "y_test_predict = rf.predict(X_test)\n",
    "print(\"The result in testing dataset is:\")\n",
    "print(\"accuracy is \", accuracy_score(y_test, y_test_predict), \"and recall is\",recall_score(y_test, y_test_predict))\n",
    "print(\"AUC score is \", roc_auc_score(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see after applying SMOTE, outr accuracy rate increases no matter we use basic logistic regression or random forest. To test it, we can applied more different classification algorithm in machine learning to prove it. The point is even we use simple logistic regression, SMOTE helps. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
