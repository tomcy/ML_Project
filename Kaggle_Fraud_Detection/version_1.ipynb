{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline of this code:\n",
    "1. undersample non fraud dataset and make a balanced dataset as 1:1 to train the model\n",
    "2. use precision, recall, f1 and kappa to choose the best parameter c for regularization\n",
    "3. try different ratio of data to get more accurate and stable results\n",
    "4. using SMOTE to generate synthetic data points, accuracy is 0.944 and recall is 0.914 by logistic regression\n",
    "5. try to use random forest and orignal 1:1 ratio data set\n",
    "6. try to combine random forest and data after SMOTE\n",
    "7. eventually achieve recall rate is 1 and accuracy is 0.999871035061 and AUC is 0.99987081469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, f1_score, cohen_kappa_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non fraud rate:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9982725143693799"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./creditcard.csv\")\n",
    "data.head()\n",
    "print(\"Non fraud rate:\")\n",
    "len(data.loc[data.loc[:, 'Class'] == 0, :]) / len(data.loc[:, 'Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that the data is really biased toward non fraud data points. Therefore, in order to get robust result we have to resample the data. Firstly, we try to under sample the non-fraud data points to get 1:1 ratio dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of fraud:  492\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142700</th>\n",
       "      <td>84885.0</td>\n",
       "      <td>1.282544</td>\n",
       "      <td>0.343615</td>\n",
       "      <td>0.155205</td>\n",
       "      <td>0.608999</td>\n",
       "      <td>-0.199510</td>\n",
       "      <td>-0.868578</td>\n",
       "      <td>0.075792</td>\n",
       "      <td>-0.178470</td>\n",
       "      <td>0.063093</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.311994</td>\n",
       "      <td>-0.899064</td>\n",
       "      <td>0.085810</td>\n",
       "      <td>-0.016022</td>\n",
       "      <td>0.262946</td>\n",
       "      <td>0.116895</td>\n",
       "      <td>-0.023705</td>\n",
       "      <td>0.027006</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148328</th>\n",
       "      <td>89718.0</td>\n",
       "      <td>-0.151769</td>\n",
       "      <td>0.773317</td>\n",
       "      <td>0.217510</td>\n",
       "      <td>-0.990222</td>\n",
       "      <td>1.273956</td>\n",
       "      <td>-0.133331</td>\n",
       "      <td>1.220424</td>\n",
       "      <td>-0.233007</td>\n",
       "      <td>0.057706</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.328350</td>\n",
       "      <td>-0.668780</td>\n",
       "      <td>-0.049670</td>\n",
       "      <td>-0.043039</td>\n",
       "      <td>-0.537554</td>\n",
       "      <td>0.123571</td>\n",
       "      <td>0.180082</td>\n",
       "      <td>0.013355</td>\n",
       "      <td>11.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283061</th>\n",
       "      <td>171347.0</td>\n",
       "      <td>-1.159551</td>\n",
       "      <td>-1.267010</td>\n",
       "      <td>-0.664532</td>\n",
       "      <td>-0.333422</td>\n",
       "      <td>2.151709</td>\n",
       "      <td>-0.265853</td>\n",
       "      <td>0.929949</td>\n",
       "      <td>-0.313039</td>\n",
       "      <td>-0.008446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197362</td>\n",
       "      <td>1.261170</td>\n",
       "      <td>1.161797</td>\n",
       "      <td>-0.329007</td>\n",
       "      <td>0.094537</td>\n",
       "      <td>-0.122662</td>\n",
       "      <td>0.069772</td>\n",
       "      <td>-0.314031</td>\n",
       "      <td>84.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58107</th>\n",
       "      <td>48234.0</td>\n",
       "      <td>-2.118490</td>\n",
       "      <td>2.251822</td>\n",
       "      <td>1.969822</td>\n",
       "      <td>2.925761</td>\n",
       "      <td>-1.280051</td>\n",
       "      <td>0.295898</td>\n",
       "      <td>-0.624246</td>\n",
       "      <td>0.633182</td>\n",
       "      <td>-0.263062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112593</td>\n",
       "      <td>0.271349</td>\n",
       "      <td>0.054213</td>\n",
       "      <td>0.719109</td>\n",
       "      <td>-0.267708</td>\n",
       "      <td>0.072916</td>\n",
       "      <td>-0.863805</td>\n",
       "      <td>-0.067662</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235474</th>\n",
       "      <td>148402.0</td>\n",
       "      <td>-1.303719</td>\n",
       "      <td>1.105589</td>\n",
       "      <td>-3.379120</td>\n",
       "      <td>-0.383921</td>\n",
       "      <td>0.847778</td>\n",
       "      <td>1.403939</td>\n",
       "      <td>-0.032561</td>\n",
       "      <td>-1.787864</td>\n",
       "      <td>-1.020774</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.554459</td>\n",
       "      <td>2.550605</td>\n",
       "      <td>-0.325024</td>\n",
       "      <td>-1.667217</td>\n",
       "      <td>-1.654295</td>\n",
       "      <td>-0.054046</td>\n",
       "      <td>0.206256</td>\n",
       "      <td>0.047341</td>\n",
       "      <td>233.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "142700   84885.0  1.282544  0.343615  0.155205  0.608999 -0.199510 -0.868578   \n",
       "148328   89718.0 -0.151769  0.773317  0.217510 -0.990222  1.273956 -0.133331   \n",
       "283061  171347.0 -1.159551 -1.267010 -0.664532 -0.333422  2.151709 -0.265853   \n",
       "58107    48234.0 -2.118490  2.251822  1.969822  2.925761 -1.280051  0.295898   \n",
       "235474  148402.0 -1.303719  1.105589 -3.379120 -0.383921  0.847778  1.403939   \n",
       "\n",
       "              V7        V8        V9  ...         V21       V22       V23  \\\n",
       "142700  0.075792 -0.178470  0.063093  ...   -0.311994 -0.899064  0.085810   \n",
       "148328  1.220424 -0.233007  0.057706  ...   -0.328350 -0.668780 -0.049670   \n",
       "283061  0.929949 -0.313039 -0.008446  ...    0.197362  1.261170  1.161797   \n",
       "58107  -0.624246  0.633182 -0.263062  ...    0.112593  0.271349  0.054213   \n",
       "235474 -0.032561 -1.787864 -1.020774  ...   -0.554459  2.550605 -0.325024   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "142700 -0.016022  0.262946  0.116895 -0.023705  0.027006    1.29      0  \n",
       "148328 -0.043039 -0.537554  0.123571  0.180082  0.013355   11.99      0  \n",
       "283061 -0.329007  0.094537 -0.122662  0.069772 -0.314031   84.39      0  \n",
       "58107   0.719109 -0.267708  0.072916 -0.863805 -0.067662    0.76      0  \n",
       "235474 -1.667217 -1.654295 -0.054046  0.206256  0.047341  233.29      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# under sample non fraud\n",
    "len_fraud = len(data.loc[data.loc[:, 'Class'] == 1, :])\n",
    "print(\"number of fraud: \", len_fraud)\n",
    "# sample from non fraud to have 1:1 propotion\n",
    "sub_non_fraud = data.loc[data.loc[:, 'Class'] == 0, :].sample(len_fraud)\n",
    "sub_non_fraud.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non fraud rate:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine resample fraud and non fraud data\n",
    "data_resample = pd.concat([sub_non_fraud, data.loc[data.loc[:, 'Class'] == 1, :]])\n",
    "print(\"Non fraud rate:\")\n",
    "len(data_resample.loc[data_resample.loc[:, 'Class'] == 0, :]) / len(data_resample.loc[:, 'Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After under sample the non-fraud data points, we try logistic regression first as out benchmark. In this practice, we care not only the accuracy rate but also the recall rate. We introduce F1 and Cohen’s kappa to compare different regularization power c.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For c= 0.001 \t recall rate is 0.958063986415  and accuracy is 0.764171760075  f1 = 0.801660541863  kappa = 0.528178323233\n",
      "For c= 0.01 \t recall rate is 0.920821294594  and accuracy is 0.933922096757  f1 = 0.931979526091  kappa = 0.86710778374\n",
      "For c= 0.1 \t recall rate is 0.902105846883  and accuracy is 0.942054283642  f1 = 0.939361869465  kappa = 0.883856840724\n",
      "For c= 1 \t recall rate is 0.910399919161  and accuracy is 0.937004040195  f1 = 0.93401401012  kappa = 0.872990971213\n",
      "For c= 10 \t recall rate is 0.915312438843  and accuracy is 0.935957733347  f1 = 0.933854859828  kappa = 0.871268818006\n",
      "For c= 100 \t recall rate is 0.914643641694  and accuracy is 0.938008909147  f1 = 0.936494466004  kappa = 0.875465334199\n"
     ]
    }
   ],
   "source": [
    "# use k fold to find the highest recall rate parameter\n",
    "X = data_resample.iloc[:, 1:29]\n",
    "Y = data_resample.loc[:, \"Class\"]\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "kf.get_n_splits(X)\n",
    "# test on different regularation power\n",
    "cs = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "for c in cs:\n",
    "    accuracy = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    kappa = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "        y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "        logreg = LogisticRegression(penalty='l1', solver='liblinear', C=c, max_iter=100)\n",
    "        logreg.fit(X_train, y_train)\n",
    "        y_test_pred = logreg.predict(X_test)\n",
    "        recall.append(recall_score(y_test, y_test_pred))\n",
    "        accuracy.append(accuracy_score(y_test, y_test_pred))\n",
    "        f1.append(f1_score(y_test, y_test_pred))\n",
    "        kappa.append(cohen_kappa_score(y_test, y_test_pred))\n",
    "    print(\"For c=\", c, \"\\t recall rate is\", np.mean(recall), \" and accuracy is\", np.mean(accuracy), \" f1 =\", np.mean(f1), \" kappa =\", np.mean(kappa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After comparing Kappa and f1 score, we choose to use c = 1 as the best parameter for regularization.  \n",
    "Then we use c=1 and resample dataset to train logistic regression and predict on the original whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94710745367306304"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use all data after resample to train the model\n",
    "X = data_resample.iloc[:, 1:29]\n",
    "Y = data_resample.loc[:, \"Class\"]\n",
    "logreg = LogisticRegression(penalty='l1', solver='liblinear', C=1, max_iter=100)\n",
    "logreg.fit(X, Y)\n",
    "# apply to original dataset\n",
    "ori_data = pd.read_csv(\"./creditcard.csv\")\n",
    "X_test = ori_data.iloc[:, 1:29]\n",
    "Y_test = ori_data.loc[:, 'Class']\n",
    "Y_test_predict = logreg.predict(X_test)\n",
    "accuracy_score(Y_test, Y_test_predict)\n",
    "recall_score(Y_test, Y_test_predict)\n",
    "# AUC score\n",
    "roc_auc_score(Y_test, Y_test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ratio of data  \n",
    "There is other method to tackle imbalance dataset. First I would like to try different ratio of fraud and non-fraud data such as 1:1.5, 1:2 and so on. Let's see what can it bring to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to try to use different ratio and output the result\n",
    "def result_by_ratio(size_non_fraud):\n",
    "    # generate different ratio of data\n",
    "    sub_non_fraud = data.loc[data.loc[:, 'Class'] == 0, :].sample(int(len_fraud*size_non_fraud))\n",
    "    data_resample = pd.concat([sub_non_fraud, data.loc[data.loc[:, 'Class'] == 1, :]])\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(\"Non fraud rate:\", len(data_resample.loc[data_resample.loc[:, 'Class'] == 0, :]) / len(data_resample.loc[:, 'Class']))\n",
    "    # use k fold to find the highest recall rate parameter\n",
    "    X = data_resample.iloc[:, 1:29]\n",
    "    Y = data_resample.loc[:, \"Class\"]\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    kf.get_n_splits(X)\n",
    "    # test on different regularation power\n",
    "    cs = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    for c in cs:\n",
    "        accuracy = []\n",
    "        recall = []\n",
    "        f1 = []\n",
    "        kappa = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "            y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "            logreg = LogisticRegression(penalty='l1', solver='liblinear', C=c, max_iter=100)\n",
    "            logreg.fit(X_train, y_train)\n",
    "            y_test_pred = logreg.predict(X_test)\n",
    "            recall.append(recall_score(y_test, y_test_pred))\n",
    "            accuracy.append(accuracy_score(y_test, y_test_pred))\n",
    "            f1.append(f1_score(y_test, y_test_pred))\n",
    "            kappa.append(cohen_kappa_score(y_test, y_test_pred))\n",
    "    chosed_c = cs[kappa.index(max(kappa))]\n",
    "    # use all data after resample to train the model\n",
    "    X = data_resample.iloc[:, 1:29]\n",
    "    Y = data_resample.loc[:, \"Class\"]\n",
    "    logreg = LogisticRegression(penalty='l1', solver='liblinear', C=chosed_c, max_iter=100)\n",
    "    logreg.fit(X, Y)\n",
    "    # apply to original dataset\n",
    "    ori_data = pd.read_csv(\"./creditcard.csv\")\n",
    "    X_test = ori_data.iloc[:, 1:29]\n",
    "    Y_test = ori_data.loc[:, 'Class']\n",
    "    Y_test_predict = logreg.predict(X_test)\n",
    "    print(\"when ratio is 1:\", size_non_fraud, \"accuracy is \", \\\n",
    "              accuracy_score(Y_test, Y_test_predict), \"and recall is\",recall_score(Y_test, Y_test_predict), \\\n",
    "          \" and ROC score is\", roc_auc_score(Y_test, Y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Non fraud rate: 0.5\n",
      "when ratio is 1: 1 accuracy is  0.970580077035 and recall is 0.90243902439  and ROC score is 0.936568508907\n",
      "-----------------------------------------------\n",
      "Non fraud rate: 0.6\n",
      "when ratio is 1: 1.5 accuracy is  0.981457618668 and recall is 0.908536585366  and ROC score is 0.94506019603\n",
      "-----------------------------------------------\n",
      "Non fraud rate: 0.6666666666666666\n",
      "when ratio is 1: 2 accuracy is  0.99102901263 and recall is 0.867886178862  and ROC score is 0.929564143543\n",
      "-----------------------------------------------\n",
      "Non fraud rate: 0.7142857142857143\n",
      "when ratio is 1: 2.5 accuracy is  0.994792965061 and recall is 0.857723577236  and ROC score is 0.926376868723\n",
      "-----------------------------------------------\n",
      "Non fraud rate: 0.75\n",
      "when ratio is 1: 3 accuracy is  0.550460487277 and recall is 0.955284552846  and ROC score is 0.752522251099\n",
      "-----------------------------------------------\n",
      "Non fraud rate: 0.7777777777777778\n",
      "when ratio is 1: 3.5 accuracy is  0.994329493306 and recall is 0.871951219512  and ROC score is 0.933246242681\n",
      "-----------------------------------------------\n",
      "Non fraud rate: 0.8\n",
      "when ratio is 1: 4 accuracy is  0.991671553017 and recall is 0.888211382114  and ROC score is 0.940030985185\n"
     ]
    }
   ],
   "source": [
    "ratio_list = [1, 1.5, 2, 2.5, 3, 3.5, 4]\n",
    "for ratio in ratio_list:\n",
    "    result_by_ratio(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the above result, we saw some dramatic drop in accuracy when we use some sample size and seems there is no good ratio of data that can help us to cure the imbalanced data. \n",
    "#### now I would like to generate Synthetic Samples by SMOTE  \n",
    "The SMOTE is a synthetic minority over-sampling technique to over sample the data. You may refer to [Imbalace learning](https://github.com/scikit-learn-contrib/imbalanced-learn) to have more info and different method. Here I use the basic one. For proformance purpose, I reduce the nonfraud data point to quarter to reduce the time to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sub_non_fraud = data.loc[data.loc[:, 'Class'] == 0, :].sample(int(len(data.loc[:, 'Class']) / 2))\n",
    "data_resample = pd.concat([sub_non_fraud, data.loc[data.loc[:, 'Class'] == 1, :]])\n",
    "X = data_resample.iloc[:, 1:29]\n",
    "y = data_resample.loc[:, \"Class\"]\n",
    "sm = SMOTE(kind='regular')\n",
    "X_resampled, y_resampled = sm.fit_sample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X (284806, 28)\n",
      "Size of y (284806,)\n",
      "Size of fraud (142403,)\n"
     ]
    }
   ],
   "source": [
    "# size of X and y after SMOTE\n",
    "print(\"Size of X\", X_resampled.shape)\n",
    "print(\"Size of y\", y_resampled.shape)\n",
    "print(\"Size of fraud\", y_resampled[y_resampled == 1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After SMOTE, we use logistic regression based on resample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_resampled = pd.DataFrame(X_resampled)\n",
    "y_resampled = pd.DataFrame(y_resampled)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.3, random_state = 42)\n",
    "c = [0.01, 0.1, 1, 10]\n",
    "logreg = LogisticRegressionCV(penalty='l2', solver='sag', Cs=c, refit=True, cv=10, max_iter=100)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_test_predict = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is  0.945413262798 and recall is 0.915253050844\n",
      "AUC score is  0.945509582836\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy is \", accuracy_score(y_test, y_test_predict), \"and recall is\",recall_score(y_test, y_test_predict))\n",
    "print(\"AUC score is \", roc_auc_score(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The result seems similar to previous one and doesn't improve a lot. Let's try different learning algorithm for example Random Forest. We use resample dataset by SMOTE combined with random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is  0.999719107699 and recall is 1.0\n",
      "AUC score is  0.999718210638\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "y_test_predict = rf.predict(X_test)\n",
    "print(\"accuracy is \", accuracy_score(y_test, y_test_predict), \"and recall is\",recall_score(y_test, y_test_predict))\n",
    "print(\"AUC score is \", roc_auc_score(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we get recall rate is almost 1 and really high AUC score. \n",
    "The result seems incredibly good. Does the result come from SMOTE or Random Forest. Let's use only the original data to train the random forest again without SMOTE resample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy rate is  0.970839902109\n",
      "recall rate is  0.967479674797\n",
      "AUC score is  0.969162695848\n"
     ]
    }
   ],
   "source": [
    "# under sample non fraud\n",
    "len_fraud = len(data.loc[data.loc[:, 'Class'] == 1, :])\n",
    "# sample from non fraud to have 50/50 propotion\n",
    "sub_non_fraud = data.loc[data.loc[:, 'Class'] == 0, :].sample(len_fraud)\n",
    "data_resample = pd.concat([sub_non_fraud, data.loc[data.loc[:, 'Class'] == 1, :]])\n",
    "X = data_resample.iloc[:, 1:29]\n",
    "y = data_resample.loc[:, \"Class\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "rf2 = RandomForestClassifier(n_estimators=100)\n",
    "rf2.fit(X_train, y_train)\n",
    "\n",
    "ori_data = pd.read_csv(\"./creditcard.csv\")\n",
    "X_test = ori_data.iloc[:, 1:29]\n",
    "y_test = ori_data.loc[:, 'Class']\n",
    "y_test_predict = rf2.predict(X_test)\n",
    "print(\"accuracy rate is \", accuracy_score(y_test, y_test_predict))\n",
    "print(\"recall rate is \", recall_score(y_test, y_test_predict))\n",
    "print(\"AUC score is \", roc_auc_score(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we conclude that combining SMOTE and Random forest, we can get a really good result. If we use only random forest, the result seems decent. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}